{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df5c981d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95a46509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "# === Image Transform ===\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# === Helper function to check if a file is an image ===\n",
    "def is_image_file(filename):\n",
    "    return any(filename.lower().endswith(ext) for ext in ['.png', '.jpg', '.jpeg', '.bmp'])\n",
    "\n",
    "# === Dataset Class for Real and Cartoon Images ===\n",
    "class CartoonDataset(Dataset):\n",
    "    def __init__(self, real_dir, cartoon_dir, transform=None, shuffle=True):\n",
    "        self.real_images = [os.path.join(real_dir, img) for img in os.listdir(real_dir) if is_image_file(img)]\n",
    "        self.cartoon_images = [os.path.join(cartoon_dir, img) for img in os.listdir(cartoon_dir) if is_image_file(img)]\n",
    "        self.transform = transform\n",
    "\n",
    "        # Shuffle the lists to ensure random pairing\n",
    "        if shuffle:\n",
    "            random.shuffle(self.real_images)\n",
    "            random.shuffle(self.cartoon_images)\n",
    "        \n",
    "        # Make sure both lists are equal in size (minimum length)\n",
    "        self.length = min(len(self.real_images), len(self.cartoon_images))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        real_image = Image.open(self.real_images[idx]).convert('RGB')\n",
    "        cartoon_image = Image.open(self.cartoon_images[idx]).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            real_image = self.transform(real_image)\n",
    "            cartoon_image = self.transform(cartoon_image)\n",
    "        \n",
    "        return real_image, cartoon_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50e79dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Define dataset\n",
    "real_dir = \"archive (16)/PytorchCtoonGAN/dataset/train_photo\"\n",
    "cartoon_dir = \"archive (16)/PytorchCtoonGAN/dataset/Hayao/style\"\n",
    "dataset = CartoonDataset(real_dir, cartoon_dir, transform=transform)\n",
    "\n",
    "# Total size of dataset\n",
    "total_size = len(dataset)\n",
    "\n",
    "# Define split proportions\n",
    "train_size = int(0.7 * total_size)\n",
    "val_size = int(0.15 * total_size)\n",
    "test_size = total_size - train_size - val_size  # Remaining goes to test\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "defe7437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import vgg19\n",
    "from torchvision import transforms\n",
    "\n",
    "# === Hinge Loss for Discriminator ===\n",
    "def discriminator_hinge_loss(real_pred, fake_pred):\n",
    "    real_loss = torch.mean(F.relu(1.0 - real_pred))\n",
    "    fake_loss = torch.mean(F.relu(1.0 + fake_pred))\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "# === Hinge Loss for Generator ===\n",
    "def generator_hinge_loss(fake_pred):\n",
    "    return -torch.mean(fake_pred)\n",
    "\n",
    "# === Perceptual Content Loss using VGG19 ===\n",
    "class VGGContentLoss(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(VGGContentLoss, self).__init__()\n",
    "        vgg = vgg19(pretrained=True).features[:21].eval()\n",
    "        for param in vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.vgg = vgg.to(device)\n",
    "        self.norm = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        input = self.norm(input)\n",
    "        target = self.norm(target)\n",
    "        return F.l1_loss(self.vgg(input), self.vgg(target))\n",
    "\n",
    "# === Optional Style Loss ===\n",
    "def gram_matrix(features):\n",
    "    (b, c, h, w) = features.size()\n",
    "    features = features.view(b, c, h * w)\n",
    "    G = torch.bmm(features, features.transpose(1, 2))\n",
    "    return G / (c * h * w)\n",
    "\n",
    "class StyleLoss(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        vgg = vgg19(pretrained=True).features[:21].eval()\n",
    "        for param in vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.vgg = vgg.to(device)\n",
    "        self.norm = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        input = self.norm(input)\n",
    "        target = self.norm(target)\n",
    "        input_features = self.vgg(input)\n",
    "        target_features = self.vgg(target)\n",
    "        return F.l1_loss(gram_matrix(input_features), gram_matrix(target_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b39fb511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.utils.spectral_norm as spectral_norm\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            spectral_norm(nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            spectral_norm(nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            spectral_norm(nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            spectral_norm(nn.Conv2d(256, 1, kernel_size=4, stride=1, padding=1))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75ce0928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=3),\n",
    "            nn.InstanceNorm2d(in_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=3),\n",
    "            nn.InstanceNorm2d(in_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # Initial convolution block\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(3, 64, kernel_size=7),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # Downsampling\n",
    "        self.down1 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.down2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # Residual blocks\n",
    "        self.res_blocks = nn.Sequential(*[ResidualBlock(256) for _ in range(6)])\n",
    "\n",
    "        # Upsampling\n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.up2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # Final output block\n",
    "        self.final = nn.Sequential(\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(64, 3, kernel_size=7),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial(x)\n",
    "        x = self.down1(x)\n",
    "        x = self.down2(x)\n",
    "        x = self.res_blocks(x)\n",
    "        x = self.up1(x)\n",
    "        x = self.up2(x)\n",
    "        return self.final(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d70d0829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], D Loss: 0.9600, G Loss: 6.3195, D Accuracy: 37.33%\n",
      "Validation G Loss: 5.0150, D Accuracy: 33.35%\n",
      "✅ Saved Best Model at Epoch 1 with Val G Loss: 5.0150\n",
      "Epoch [2/50], D Loss: 0.5217, G Loss: 6.0691, D Accuracy: 52.92%\n",
      "Validation G Loss: 4.9940, D Accuracy: 55.72%\n",
      "✅ Saved Best Model at Epoch 2 with Val G Loss: 4.9940\n",
      "Epoch [3/50], D Loss: 0.7475, G Loss: 5.7772, D Accuracy: 67.17%\n",
      "Validation G Loss: 5.6219, D Accuracy: 69.50%\n",
      "⏳ No improvement for 1 epoch(s).\n",
      "Epoch [4/50], D Loss: 0.7175, G Loss: 6.4150, D Accuracy: 65.07%\n",
      "Validation G Loss: 4.5260, D Accuracy: 25.81%\n",
      "✅ Saved Best Model at Epoch 4 with Val G Loss: 4.5260\n",
      "Epoch [5/50], D Loss: 0.3328, G Loss: 6.7222, D Accuracy: 68.25%\n",
      "Validation G Loss: 5.5628, D Accuracy: 60.32%\n",
      "⏳ No improvement for 1 epoch(s).\n",
      "Epoch [6/50], D Loss: 0.2809, G Loss: 5.7846, D Accuracy: 60.14%\n",
      "Validation G Loss: 5.7557, D Accuracy: 66.88%\n",
      "⏳ No improvement for 2 epoch(s).\n",
      "Epoch [7/50], D Loss: 0.5260, G Loss: 5.9098, D Accuracy: 64.11%\n",
      "Validation G Loss: 5.5936, D Accuracy: 70.59%\n",
      "⏳ No improvement for 3 epoch(s).\n",
      "Epoch [8/50], D Loss: 0.1316, G Loss: 6.9279, D Accuracy: 67.93%\n",
      "Validation G Loss: 5.7452, D Accuracy: 64.28%\n",
      "⏳ No improvement for 4 epoch(s).\n",
      "Epoch [9/50], D Loss: 0.8429, G Loss: 7.0087, D Accuracy: 63.59%\n",
      "Validation G Loss: 5.4537, D Accuracy: 59.79%\n",
      "⏳ No improvement for 5 epoch(s).\n",
      "🛑 Early stopping triggered after 9 epochs.\n",
      "🎉 Training Complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 50\n",
    "n_critic = 2\n",
    "best_val_g_loss = float('inf')  # For saving best model based on generator validation loss\n",
    "patience = 5                    # Number of epochs to wait for improvement\n",
    "epochs_without_improvement = 0 # Early stopping counter\n",
    "\n",
    "# Initialize models\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# Loss functions\n",
    "content_loss_fn = VGGContentLoss(device)\n",
    "style_loss_fn = StyleLoss(device)\n",
    "use_style_loss = True  # toggle this to False if not needed\n",
    "\n",
    "# Optimizers\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=5e-5, betas=(0.5, 0.999))\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "\n",
    "# Learning rate schedulers\n",
    "scheduler_D = torch.optim.lr_scheduler.StepLR(optimizer_D, step_size=10, gamma=0.5)\n",
    "scheduler_G = torch.optim.lr_scheduler.StepLR(optimizer_G, step_size=10, gamma=0.5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i, (real, cartoon) in enumerate(train_loader):\n",
    "        real, cartoon = real.to(device), cartoon.to(device)\n",
    "\n",
    "        # === Train Discriminator ===\n",
    "        for _ in range(n_critic):\n",
    "            optimizer_D.zero_grad()\n",
    "            real_pred = discriminator(cartoon)\n",
    "            fake_cartoon = generator(real).detach()\n",
    "            fake_pred = discriminator(fake_cartoon)\n",
    "            d_loss = discriminator_hinge_loss(real_pred, fake_pred)\n",
    "            d_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(discriminator.parameters(), 1.0)\n",
    "            optimizer_D.step()\n",
    "\n",
    "        # === Approximate D Accuracy ===\n",
    "        real_correct = (real_pred > 0.6).sum().item()\n",
    "        fake_correct = (fake_pred < -0.6).sum().item()\n",
    "        correct += real_correct + fake_correct\n",
    "        total += real_pred.numel() + fake_pred.numel()\n",
    "\n",
    "        # === Train Generator ===\n",
    "        optimizer_G.zero_grad()\n",
    "        fake_cartoon = generator(real)\n",
    "        fake_pred = discriminator(fake_cartoon)\n",
    "\n",
    "        g_loss = generator_hinge_loss(fake_pred)\n",
    "        g_loss += content_loss_fn(fake_cartoon, cartoon)\n",
    "        if use_style_loss:\n",
    "            g_loss += 50.0 * style_loss_fn(fake_cartoon, cartoon)\n",
    "\n",
    "        g_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(generator.parameters(), 1.0)\n",
    "        optimizer_G.step()\n",
    "\n",
    "    train_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}, D Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    # === Validation ===\n",
    "    generator.eval()\n",
    "    val_g_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for real, cartoon in val_loader:\n",
    "            real, cartoon = real.to(device), cartoon.to(device)\n",
    "            fake_cartoon = generator(real)\n",
    "            fake_pred = discriminator(fake_cartoon)\n",
    "            val_real_pred = discriminator(cartoon)\n",
    "\n",
    "            loss = generator_hinge_loss(fake_pred) + content_loss_fn(fake_cartoon, cartoon)\n",
    "            if use_style_loss:\n",
    "                loss += 50.0 * style_loss_fn(fake_cartoon, cartoon)\n",
    "\n",
    "            val_g_loss += loss\n",
    "\n",
    "            val_real_correct = (val_real_pred > 0.6).sum().item()\n",
    "            val_fake_correct = (fake_pred < -0.6).sum().item()\n",
    "            val_correct += val_real_correct + val_fake_correct\n",
    "            val_total += val_real_pred.numel() + fake_pred.numel()\n",
    "\n",
    "    val_g_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    print(f\"Validation G Loss: {val_g_loss.item():.4f}, D Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # ✅ Save best model based on lowest generator validation loss\n",
    "    if val_g_loss.item() < best_val_g_loss:\n",
    "        best_val_g_loss = val_g_loss.item()\n",
    "        epochs_without_improvement = 0\n",
    "        torch.save(generator.state_dict(), \"best_Generator2.pth\")\n",
    "        torch.save(discriminator.state_dict(), \"best_discriminator2.pth\")\n",
    "        print(f\"✅ Saved Best Model at Epoch {epoch+1} with Val G Loss: {val_g_loss.item():.4f}\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        print(f\"⏳ No improvement for {epochs_without_improvement} epoch(s).\")\n",
    "\n",
    "    # 🛑 Early Stopping\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"🛑 Early stopping triggered after {epoch+1} epochs.\")\n",
    "        break\n",
    "\n",
    "    scheduler_D.step()\n",
    "    scheduler_G.step()\n",
    "\n",
    "print(\"🎉 Training Complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3955d193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cartoonized test images saved in 'test_outputs1' folder.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Load the generator model\n",
    "generator = Generator().to(device)\n",
    "generator.load_state_dict(torch.load(\"best_Generator2.pth\"))\n",
    "generator.eval()\n",
    "\n",
    "# Directory to save outputs\n",
    "output_dir = \"test_outputs1\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Testing loop\n",
    "with torch.no_grad():\n",
    "    for idx, (real, _) in enumerate(test_loader):\n",
    "        real = real.to(device)\n",
    "        fake_cartoon = generator(real)\n",
    "\n",
    "        # Denormalize output before saving: [-1, 1] -> [0, 1]\n",
    "        fake_cartoon = (fake_cartoon + 1) / 2\n",
    "        real = (real + 1) / 2\n",
    "\n",
    "        # Save the real and cartoonized images side by side\n",
    "        for i in range(real.size(0)):\n",
    "            save_image(torch.cat([real[i], fake_cartoon[i]], dim=2),\n",
    "                       os.path.join(output_dir, f\"sample_{idx * test_loader.batch_size + i}.png\"))\n",
    "\n",
    "print(\"✅ Cartoonized test images saved in 'test_outputs1' folder.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
